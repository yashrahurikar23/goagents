# üåä Phase 1: Streaming Support Implementation Guide

**Timeline:** Week 1-2  
**Complexity:** Medium-High  
**Impact:** Very High ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

## üìã Overview

Implement token-by-token streaming for all LLM providers and agents. This enables real-time responses, better UX, and progress indicators.

**Key Deliverables:**
- Core streaming infrastructure
- Streaming for 4 LLM providers
- Streaming for 3 agent types
- 30+ tests
- 4+ examples

---

## üèóÔ∏è Step 1: Core Streaming Infrastructure (Day 1)

### Task 1.1: Create Core Streaming Types

**File:** `core/streaming.go`

```go
package core

import (
	"context"
	"time"
)

// StreamChunk represents a single piece of streamed content
type StreamChunk struct {
	// Content is the text content of this chunk
	Content string
	
	// Delta is the incremental text (for token-by-token streaming)
	Delta string
	
	// Index is the position in the stream
	Index int
	
	// FinishReason indicates why the stream ended
	// Values: "stop", "length", "tool_calls", "content_filter", ""
	FinishReason string
	
	// Metadata contains provider-specific information
	Metadata map[string]interface{}
	
	// Timestamp when this chunk was created
	Timestamp time.Time
}

// StreamEvent represents an event during agent execution
type StreamEvent struct {
	// Type of event: "token", "thought", "tool_start", "tool_end", "answer", "complete", "error"
	Type string
	
	// Content is the event content (token text, thought, tool result, etc.)
	Content string
	
	// Data contains structured event data
	Data map[string]interface{}
	
	// Error if Type is "error"
	Error error
	
	// Timestamp when this event occurred
	Timestamp time.Time
}

// StreamingLLM extends LLM with streaming capabilities
type StreamingLLM interface {
	LLM
	
	// ChatStream generates a response as a stream of chunks
	ChatStream(ctx context.Context, messages []Message, opts ...interface{}) (<-chan StreamChunk, error)
	
	// CompleteStream generates a completion as a stream of chunks
	CompleteStream(ctx context.Context, prompt string, opts ...interface{}) (<-chan StreamChunk, error)
}

// StreamingAgent extends Agent with streaming capabilities
type StreamingAgent interface {
	Agent
	
	// RunStream executes the agent and streams events in real-time
	RunStream(ctx context.Context, input string) (<-chan StreamEvent, error)
}

// EventType constants
const (
	EventTypeToken     = "token"      // Individual token from LLM
	EventTypeThought   = "thought"    // Reasoning step (ReAct)
	EventTypeToolStart = "tool_start" // Tool execution starting
	EventTypeToolEnd   = "tool_end"   // Tool execution complete
	EventTypeAnswer    = "answer"     // Final answer
	EventTypeComplete  = "complete"   // Execution complete
	EventTypeError     = "error"      // Error occurred
)
```

**Tests:** `core/streaming_test.go`

```go
package core

import (
	"testing"
	"time"
)

func TestStreamChunk(t *testing.T) {
	chunk := StreamChunk{
		Content:      "Hello",
		Delta:        "Hello",
		Index:        0,
		FinishReason: "",
		Metadata:     map[string]interface{}{"model": "gpt-4"},
		Timestamp:    time.Now(),
	}
	
	if chunk.Content != "Hello" {
		t.Errorf("Expected Content 'Hello', got '%s'", chunk.Content)
	}
}

func TestStreamEvent(t *testing.T) {
	event := StreamEvent{
		Type:      EventTypeToken,
		Content:   "test",
		Data:      map[string]interface{}{"index": 0},
		Error:     nil,
		Timestamp: time.Now(),
	}
	
	if event.Type != EventTypeToken {
		t.Errorf("Expected Type 'token', got '%s'", event.Type)
	}
}

func TestEventTypeConstants(t *testing.T) {
	tests := []struct {
		constant string
		expected string
	}{
		{EventTypeToken, "token"},
		{EventTypeThought, "thought"},
		{EventTypeToolStart, "tool_start"},
		{EventTypeToolEnd, "tool_end"},
		{EventTypeAnswer, "answer"},
		{EventTypeComplete, "complete"},
		{EventTypeError, "error"},
	}
	
	for _, tt := range tests {
		if tt.constant != tt.expected {
			t.Errorf("Expected constant '%s', got '%s'", tt.expected, tt.constant)
		}
	}
}
```

---

## üîå Step 2: OpenAI Streaming (Day 2)

### Task 2.1: Implement OpenAI Streaming

**File:** `llm/openai/client.go` (add streaming methods)

```go
// ChatStream implements core.StreamingLLM
func (c *Client) ChatStream(ctx context.Context, messages []core.Message, opts ...interface{}) (<-chan core.StreamChunk, error) {
	chunks := make(chan core.StreamChunk)
	
	// Convert messages
	openaiMessages := c.convertMessages(messages)
	
	// Create streaming request
	req := ChatCompletionRequest{
		Model:    c.model,
		Messages: openaiMessages,
		Stream:   true, // Enable streaming
	}
	
	// Apply options
	for _, opt := range opts {
		if temp, ok := opt.(float64); ok {
			req.Temperature = &temp
		}
		// ... other options
	}
	
	go func() {
		defer close(chunks)
		
		// Create HTTP request
		body, _ := json.Marshal(req)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/chat/completions", bytes.NewReader(body))
		if err != nil {
			chunks <- core.StreamChunk{Error: err}
			return
		}
		
		// Set headers
		httpReq.Header.Set("Content-Type", "application/json")
		httpReq.Header.Set("Authorization", "Bearer "+c.apiKey)
		
		// Send request
		resp, err := c.httpClient.Do(httpReq)
		if err != nil {
			chunks <- core.StreamChunk{Error: err}
			return
		}
		defer resp.Body.Close()
		
		// Parse SSE (Server-Sent Events) stream
		scanner := bufio.NewScanner(resp.Body)
		index := 0
		
		for scanner.Scan() {
			line := scanner.Text()
			
			// SSE format: "data: {...}"
			if !strings.HasPrefix(line, "data: ") {
				continue
			}
			
			data := strings.TrimPrefix(line, "data: ")
			
			// Check for stream end
			if data == "[DONE]" {
				break
			}
			
			// Parse JSON
			var streamResp ChatCompletionStreamResponse
			if err := json.Unmarshal([]byte(data), &streamResp); err != nil {
				continue
			}
			
			// Extract delta
			if len(streamResp.Choices) > 0 {
				delta := streamResp.Choices[0].Delta
				finishReason := streamResp.Choices[0].FinishReason
				
				chunk := core.StreamChunk{
					Delta:        delta.Content,
					Index:        index,
					FinishReason: finishReason,
					Timestamp:    time.Now(),
					Metadata: map[string]interface{}{
						"model": streamResp.Model,
						"id":    streamResp.ID,
					},
				}
				
				chunks <- chunk
				index++
			}
		}
		
		if err := scanner.Err(); err != nil {
			chunks <- core.StreamChunk{Error: err}
		}
	}()
	
	return chunks, nil
}
```

**New Type:** `llm/openai/types.go`

```go
// ChatCompletionStreamResponse for streaming responses
type ChatCompletionStreamResponse struct {
	ID      string                        `json:"id"`
	Object  string                        `json:"object"`
	Created int64                         `json:"created"`
	Model   string                        `json:"model"`
	Choices []ChatCompletionStreamChoice  `json:"choices"`
}

type ChatCompletionStreamChoice struct {
	Index        int                  `json:"index"`
	Delta        ChatCompletionDelta  `json:"delta"`
	FinishReason string              `json:"finish_reason"`
}

type ChatCompletionDelta struct {
	Role    string `json:"role,omitempty"`
	Content string `json:"content,omitempty"`
}
```

**Tests:** `llm/openai/streaming_test.go`

```go
func TestChatStream(t *testing.T) {
	// Create mock SSE server
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		w.Header().Set("Cache-Control", "no-cache")
		w.Header().Set("Connection", "keep-alive")
		
		// Send chunks
		chunks := []string{"Hello", " world", "!"}
		for i, chunk := range chunks {
			resp := ChatCompletionStreamResponse{
				ID:    "chatcmpl-123",
				Model: "gpt-4",
				Choices: []ChatCompletionStreamChoice{
					{
						Index: i,
						Delta: ChatCompletionDelta{Content: chunk},
					},
				},
			}
			data, _ := json.Marshal(resp)
			fmt.Fprintf(w, "data: %s\n\n", data)
			w.(http.Flusher).Flush()
		}
		
		// Send done signal
		fmt.Fprintf(w, "data: [DONE]\n\n")
	}))
	defer server.Close()
	
	client := New(
		WithAPIKey("test-key"),
		WithBaseURL(server.URL),
	)
	
	ctx := context.Background()
	messages := []core.Message{
		{Role: "user", Content: "Hello"},
	}
	
	stream, err := client.ChatStream(ctx, messages)
	if err != nil {
		t.Fatalf("ChatStream failed: %v", err)
	}
	
	var content string
	for chunk := range stream {
		if chunk.Error != nil {
			t.Fatalf("Stream error: %v", chunk.Error)
		}
		content += chunk.Delta
	}
	
	expected := "Hello world!"
	if content != expected {
		t.Errorf("Expected '%s', got '%s'", expected, content)
	}
}

func TestChatStreamCancellation(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Content-Type", "text/event-stream")
		
		// Send chunks slowly
		for i := 0; i < 100; i++ {
			fmt.Fprintf(w, "data: {\"choices\":[{\"delta\":{\"content\":\"test\"}}]}\n\n")
			w.(http.Flusher).Flush()
			time.Sleep(50 * time.Millisecond)
		}
	}))
	defer server.Close()
	
	client := New(WithAPIKey("test"), WithBaseURL(server.URL))
	
	ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
	defer cancel()
	
	messages := []core.Message{{Role: "user", Content: "test"}}
	stream, _ := client.ChatStream(ctx, messages)
	
	count := 0
	for range stream {
		count++
	}
	
	// Should have been cancelled early
	if count >= 100 {
		t.Errorf("Expected early cancellation, got %d chunks", count)
	}
}
```

---

## ü¶ô Step 3: Ollama Streaming (Day 3)

### Task 3.1: Implement Ollama Streaming

**File:** `llm/ollama/client.go`

```go
func (c *Client) ChatStream(ctx context.Context, messages []core.Message, opts ...interface{}) (<-chan core.StreamChunk, error) {
	chunks := make(chan core.StreamChunk)
	
	req := ChatRequest{
		Model:    c.model,
		Messages: c.convertMessages(messages),
		Stream:   true,
	}
	
	go func() {
		defer close(chunks)
		
		body, _ := json.Marshal(req)
		httpReq, err := http.NewRequestWithContext(ctx, "POST", c.baseURL+"/api/chat", bytes.NewReader(body))
		if err != nil {
			chunks <- core.StreamChunk{Error: err}
			return
		}
		
		httpReq.Header.Set("Content-Type", "application/json")
		
		resp, err := c.httpClient.Do(httpReq)
		if err != nil {
			chunks <- core.StreamChunk{Error: err}
			return
		}
		defer resp.Body.Close()
		
		// Ollama streams JSON objects line by line
		scanner := bufio.NewScanner(resp.Body)
		index := 0
		
		for scanner.Scan() {
			var streamResp ChatResponse
			if err := json.Unmarshal(scanner.Bytes(), &streamResp); err != nil {
				continue
			}
			
			chunk := core.StreamChunk{
				Delta:        streamResp.Message.Content,
				Index:        index,
				FinishReason: func() string {
					if streamResp.Done {
						return "stop"
					}
					return ""
				}(),
				Timestamp: time.Now(),
				Metadata: map[string]interface{}{
					"model":    streamResp.Model,
					"done":     streamResp.Done,
				},
			}
			
			chunks <- chunk
			index++
			
			if streamResp.Done {
				break
			}
		}
		
		if err := scanner.Err(); err != nil {
			chunks <- core.StreamChunk{Error: err}
		}
	}()
	
	return chunks, nil
}
```

**Tests:** Similar to OpenAI tests but with Ollama's JSON streaming format

---

## ü§ñ Step 4: Streaming Agents (Day 4-5)

### Task 4.1: Streaming FunctionAgent

**File:** `agent/function.go`

```go
// RunStream implements core.StreamingAgent
func (a *FunctionAgent) RunStream(ctx context.Context, input string) (<-chan core.StreamEvent, error) {
	events := make(chan core.StreamEvent)
	
	// Check if LLM supports streaming
	streamingLLM, ok := a.llm.(core.StreamingLLM)
	if !ok {
		close(events)
		return nil, fmt.Errorf("LLM does not support streaming")
	}
	
	go func() {
		defer close(events)
		
		// Create messages
		messages := []core.Message{
			{Role: "user", Content: input},
		}
		
		// Add tool descriptions to system message
		systemMsg := a.buildSystemMessage()
		messages = append([]core.Message{systemMsg}, messages...)
		
		// Stream LLM response
		stream, err := streamingLLM.ChatStream(ctx, messages)
		if err != nil {
			events <- core.StreamEvent{
				Type:      core.EventTypeError,
				Error:     err,
				Timestamp: time.Now(),
			}
			return
		}
		
		var fullContent string
		for chunk := range stream {
			if chunk.Error != nil {
				events <- core.StreamEvent{
					Type:      core.EventTypeError,
					Error:     chunk.Error,
					Timestamp: time.Now(),
				}
				return
			}
			
			// Send token event
			events <- core.StreamEvent{
				Type:      core.EventTypeToken,
				Content:   chunk.Delta,
				Data:      chunk.Metadata,
				Timestamp: chunk.Timestamp,
			}
			
			fullContent += chunk.Delta
		}
		
		// Parse tool calls from content (if any)
		// ... tool execution logic ...
		
		// Send complete event
		events <- core.StreamEvent{
			Type:      core.EventTypeComplete,
			Content:   fullContent,
			Timestamp: time.Now(),
		}
	}()
	
	return events, nil
}
```

### Task 4.2: Streaming ReActAgent

**File:** `agent/react.go`

```go
func (a *ReActAgent) RunStream(ctx context.Context, input string) (<-chan core.StreamEvent, error) {
	events := make(chan core.StreamEvent)
	
	streamingLLM, ok := a.llm.(core.StreamingLLM)
	if !ok {
		close(events)
		return nil, fmt.Errorf("LLM does not support streaming")
	}
	
	go func() {
		defer close(events)
		
		messages := []core.Message{
			{Role: "system", Content: a.buildPrompt()},
			{Role: "user", Content: input},
		}
		
		for iteration := 0; iteration < a.maxIterations; iteration++ {
			// Stream thinking
			stream, err := streamingLLM.ChatStream(ctx, messages)
			if err != nil {
				events <- core.StreamEvent{Type: core.EventTypeError, Error: err}
				return
			}
			
			var thought string
			var isThinking bool
			
			for chunk := range stream {
				thought += chunk.Delta
				
				// Detect "Thought:" section
				if strings.Contains(thought, "Thought:") && !isThinking {
					isThinking = true
					events <- core.StreamEvent{
						Type:    core.EventTypeThought,
						Content: "Thinking...",
					}
				}
				
				// Stream token
				events <- core.StreamEvent{
					Type:    core.EventTypeToken,
					Content: chunk.Delta,
				}
			}
			
			// Parse action
			action, actionInput := a.parseAction(thought)
			
			if action == "Final Answer" {
				events <- core.StreamEvent{
					Type:    core.EventTypeAnswer,
					Content: actionInput,
				}
				break
			}
			
			// Execute tool
			events <- core.StreamEvent{
				Type:    core.EventTypeToolStart,
				Content: action,
				Data:    map[string]interface{}{"input": actionInput},
			}
			
			result, err := a.executeTool(ctx, action, actionInput)
			
			events <- core.StreamEvent{
				Type:    core.EventTypeToolEnd,
				Content: fmt.Sprintf("%v", result),
				Data:    map[string]interface{}{"error": err},
			}
			
			// Add observation to messages
			messages = append(messages, core.Message{
				Role:    "assistant",
				Content: thought,
			})
			messages = append(messages, core.Message{
				Role:    "user",
				Content: fmt.Sprintf("Observation: %v", result),
			})
		}
		
		events <- core.StreamEvent{Type: core.EventTypeComplete}
	}()
	
	return events, nil
}
```

---

## üìñ Step 5: Examples (Day 6)

### Example 1: Basic Streaming

**File:** `examples/streaming_basic/main.go`

```go
package main

import (
	"context"
	"fmt"
	"os"
	
	"github.com/yashrahurikar23/goagents/agent"
	"github.com/yashrahurikar23/goagents/core"
	"github.com/yashrahurikar23/goagents/llm/openai"
)

func main() {
	// Create OpenAI client
	llm := openai.New(
		openai.WithAPIKey(os.Getenv("OPENAI_API_KEY")),
		openai.WithModel("gpt-4"),
	)
	
	// Create agent
	a := agent.NewFunctionAgent(llm)
	
	ctx := context.Background()
	
	// Stream response
	stream, err := a.RunStream(ctx, "Tell me a short story about a robot")
	if err != nil {
		panic(err)
	}
	
	// Process stream
	fmt.Println("ü§ñ Response:")
	for event := range stream {
		switch event.Type {
		case core.EventTypeToken:
			// Print token immediately (no newline)
			fmt.Print(event.Content)
		case core.EventTypeError:
			fmt.Printf("\n‚ùå Error: %v\n", event.Error)
		case core.EventTypeComplete:
			fmt.Println("\n\n‚úÖ Complete!")
		}
	}
}
```

### Example 2: ReAct Streaming with Progress

**File:** `examples/streaming_react/main.go`

```go
package main

import (
	"context"
	"fmt"
	
	"github.com/yashrahurikar23/goagents/agent"
	"github.com/yashrahurikar23/goagents/core"
	"github.com/yashrahurikar23/goagents/llm/openai"
	"github.com/yashrahurikar23/goagents/tools"
)

func main() {
	llm := openai.New(...)
	
	// Create ReAct agent with tools
	a := agent.NewReActAgent(llm)
	a.AddTool(tools.NewCalculator())
	
	ctx := context.Background()
	stream, _ := a.RunStream(ctx, "What is 123 * 456?")
	
	// Process events with visual feedback
	for event := range stream {
		switch event.Type {
		case core.EventTypeThought:
			fmt.Println("\nü§î Thinking:", event.Content)
		case core.EventTypeToolStart:
			fmt.Printf("üîß Using tool: %s\n", event.Content)
		case core.EventTypeToolEnd:
			fmt.Printf("‚úÖ Result: %s\n", event.Content)
		case core.EventTypeAnswer:
			fmt.Printf("\nüí° Answer: %s\n", event.Content)
		case core.EventTypeToken:
			fmt.Print(event.Content)
		}
	}
}
```

---

## ‚úÖ Acceptance Criteria

**Streaming Infrastructure:**
- [x] StreamChunk and StreamEvent types defined
- [x] StreamingLLM and StreamingAgent interfaces defined
- [x] Event type constants defined
- [x] 5+ tests for core types

**OpenAI Streaming:**
- [x] ChatStream method implemented
- [x] SSE parsing working correctly
- [x] Context cancellation supported
- [x] 8+ tests including cancellation, errors, various models

**Ollama Streaming:**
- [x] ChatStream method implemented
- [x] JSON line streaming working
- [x] Context cancellation supported
- [x] 8+ tests

**Streaming Agents:**
- [x] FunctionAgent.RunStream implemented
- [x] ReActAgent.RunStream implemented
- [x] ConversationalAgent.RunStream implemented
- [x] All event types emitted correctly
- [x] 10+ tests per agent type

**Examples:**
- [x] Basic streaming example
- [x] ReAct streaming example
- [x] Progress indicators example
- [x] All examples working with real APIs

**Documentation:**
- [x] README updated with streaming examples
- [x] API docs for streaming interfaces
- [x] Migration guide for adding streaming

---

## üìù Testing Checklist

```bash
# Run all tests
cd goagents
go test ./core/... -v
go test ./llm/openai/... -v
go test ./llm/ollama/... -v
go test ./agent/... -v

# Run examples
cd examples/streaming_basic && go run main.go
cd examples/streaming_react && go run main.go

# Check coverage
go test ./... -cover
```

---

## üöÄ Ready to Start!

Begin with **Step 1** (Core Infrastructure) and work through sequentially. Each step builds on the previous one.

**Estimated Time:**
- Days 1-2: Core + OpenAI streaming
- Day 3: Ollama streaming
- Days 4-5: Agent streaming
- Day 6: Examples and testing

**Total: ~6 days** for a solo developer

Let's ship it! üéâ
