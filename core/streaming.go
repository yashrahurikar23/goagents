package core

import (
	"context"
	"time"
)

// StreamChunk represents a single piece of streamed content from an LLM.
// It contains both the incremental delta and accumulated content, along with
// metadata about the streaming state.
type StreamChunk struct {
	// Content is the accumulated text content up to this point.
	// This is the full text from the start of the stream to this chunk.
	Content string

	// Delta is the incremental text added in this chunk.
	// For token-by-token streaming, this is typically a single token.
	Delta string

	// Index is the position of this chunk in the stream.
	// Starts at 0 and increments with each chunk.
	Index int

	// FinishReason indicates why the stream ended (if it has ended).
	// Common values:
	//   - "" (empty): Stream is still ongoing
	//   - "stop": Natural completion
	//   - "length": Maximum token limit reached
	//   - "tool_calls": LLM wants to call a tool
	//   - "content_filter": Content was filtered by safety systems
	FinishReason string

	// Metadata contains provider-specific information about this chunk.
	// May include model name, token counts, timing information, etc.
	Metadata map[string]interface{}

	// Timestamp records when this chunk was created.
	Timestamp time.Time

	// Error contains any error that occurred during streaming.
	// If non-nil, this is likely the last chunk in the stream.
	Error error
}

// StreamEvent represents an event during agent execution.
// Agents emit different types of events as they work, allowing consumers
// to track progress and provide real-time feedback to users.
type StreamEvent struct {
	// Type indicates what kind of event this is.
	// See EventType constants for valid values.
	Type string

	// Content is the event-specific content.
	// For "token" events: the token text
	// For "thought" events: the reasoning text
	// For "tool_start" events: the tool name
	// For "tool_end" events: the tool result
	// For "answer" events: the final answer
	// For "error" events: the error message
	Content string

	// Data contains structured event data.
	// Contents vary by event type:
	// - "token": index, model, etc.
	// - "tool_start": tool name, input args
	// - "tool_end": tool name, result, error
	// - "thought": reasoning step number
	Data map[string]interface{}

	// Error contains the error if Type is "error".
	Error error

	// Timestamp records when this event occurred.
	Timestamp time.Time
}

// Event type constants define the types of events that can be emitted
// during agent execution.
const (
	// EventTypeToken represents an individual token from the LLM.
	// Emitted for each piece of text generated by the LLM.
	EventTypeToken = "token"

	// EventTypeThought represents a reasoning step in ReAct agents.
	// Contains the agent's internal reasoning about what to do next.
	EventTypeThought = "thought"

	// EventTypeToolStart indicates a tool is about to be executed.
	// Contains the tool name and input arguments.
	EventTypeToolStart = "tool_start"

	// EventTypeToolEnd indicates a tool has finished executing.
	// Contains the tool result or error.
	EventTypeToolEnd = "tool_end"

	// EventTypeAnswer represents the final answer from the agent.
	// This is the agent's complete response to the user's input.
	EventTypeAnswer = "answer"

	// EventTypeComplete indicates the agent has finished execution.
	// This is the last event in the stream.
	EventTypeComplete = "complete"

	// EventTypeError indicates an error occurred during execution.
	// Contains error details in the Error field.
	EventTypeError = "error"
)

// StreamingLLM extends the LLM interface with streaming capabilities.
// Implementations can generate responses as a stream of chunks rather than
// waiting for the complete response.
//
// Why streaming?
// - Provides real-time feedback to users
// - Enables progress indicators and cancellation
// - Improves perceived performance
// - Allows processing tokens as they arrive
type StreamingLLM interface {
	LLM

	// ChatStream generates a chat response as a stream of chunks.
	// Returns a channel that emits StreamChunk values as the LLM generates them.
	// The channel is closed when the stream completes or an error occurs.
	//
	// Context cancellation will stop the stream and close the channel.
	//
	// Example:
	//   stream, err := llm.ChatStream(ctx, messages)
	//   for chunk := range stream {
	//       if chunk.Error != nil {
	//           return chunk.Error
	//       }
	//       fmt.Print(chunk.Delta)
	//   }
	ChatStream(ctx context.Context, messages []Message, opts ...interface{}) (<-chan StreamChunk, error)

	// CompleteStream generates a completion as a stream of chunks.
	// Similar to ChatStream but for completion-style prompts.
	CompleteStream(ctx context.Context, prompt string, opts ...interface{}) (<-chan StreamChunk, error)
}

// StreamingAgent extends the Agent interface with streaming capabilities.
// Implementations emit events as they execute, providing real-time visibility
// into the agent's decision-making process.
//
// Why streaming agents?
// - Show users what the agent is thinking (ReAct pattern)
// - Display progress for multi-step tasks
// - Enable cancellation during long-running operations
// - Allow UI to update in real-time
type StreamingAgent interface {
	Agent

	// RunStream executes the agent and streams events in real-time.
	// Returns a channel that emits StreamEvent values as the agent works.
	// The channel is closed when execution completes or an error occurs.
	//
	// Event types emitted depend on the agent type:
	// - FunctionAgent: token, tool_start, tool_end, complete, error
	// - ReActAgent: thought, token, tool_start, tool_end, answer, complete, error
	// - ConversationalAgent: token, complete, error
	//
	// Context cancellation will stop execution and close the channel.
	//
	// Example:
	//   stream, err := agent.RunStream(ctx, "What is 2+2?")
	//   for event := range stream {
	//       switch event.Type {
	//       case core.EventTypeThought:
	//           fmt.Println("Thinking:", event.Content)
	//       case core.EventTypeToolStart:
	//           fmt.Println("Using tool:", event.Content)
	//       case core.EventTypeAnswer:
	//           fmt.Println("Answer:", event.Content)
	//       }
	//   }
	RunStream(ctx context.Context, input string) (<-chan StreamEvent, error)
}

// NewStreamChunk creates a new StreamChunk with the given delta content.
// This is a convenience function for creating chunks in streaming implementations.
func NewStreamChunk(delta string, index int) StreamChunk {
	return StreamChunk{
		Delta:     delta,
		Index:     index,
		Timestamp: time.Now(),
		Metadata:  make(map[string]interface{}),
	}
}

// NewStreamEvent creates a new StreamEvent with the given type and content.
// This is a convenience function for creating events in streaming agent implementations.
func NewStreamEvent(eventType, content string) StreamEvent {
	return StreamEvent{
		Type:      eventType,
		Content:   content,
		Timestamp: time.Now(),
		Data:      make(map[string]interface{}),
	}
}

// NewStreamEventWithData creates a new StreamEvent with type, content, and data.
func NewStreamEventWithData(eventType, content string, data map[string]interface{}) StreamEvent {
	return StreamEvent{
		Type:      eventType,
		Content:   content,
		Data:      data,
		Timestamp: time.Now(),
	}
}

// NewErrorEvent creates a StreamEvent representing an error.
func NewErrorEvent(err error) StreamEvent {
	return StreamEvent{
		Type:      EventTypeError,
		Content:   err.Error(),
		Error:     err,
		Timestamp: time.Now(),
		Data:      make(map[string]interface{}),
	}
}
